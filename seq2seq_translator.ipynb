{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOR57Pr8ThHpsRNZfTTihSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jungwoo1208/AI_Study/blob/main/seq2seq_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영어-한글로 만들고 싶었으나 데이터 부족으로 영어-프랑스어로 제작해봄"
      ],
      "metadata": {
        "id": "RZPqWC2jAeqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6QpnMIXI4Lid"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 100000"
      ],
      "metadata": {
        "id": "IfQUpsya4yKY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://www.manythings.org/anki/fra-eng.zip &&unzip -o fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-kKBjCj42ji",
        "outputId": "7e0addae-fc20-4a52-bce4-69a630c250da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-12 05:15:05--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8143096 (7.8M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.77M  21.4MB/s    in 0.4s    \n",
            "\n",
            "2025-08-12 05:15:06 (21.4 MB/s) - ‘fra-eng.zip’ saved [8143096/8143096]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "JfruYlNar0oj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sent):\n",
        "\n",
        "  sent = unicode_to_ascii(sent.lower())\n",
        "  sent= re.sub(r\"([?.!,?`])\",r\" \\1\",sent)\n",
        "  sent= re.sub(r\"[^a-zA-Z!,?`]+\",\" \",sent)\n",
        "  sent = re.sub(r\"\\s+\", \" \",sent)\n",
        "\n",
        "  return sent"
      ],
      "metadata": {
        "id": "_Y5lsv4t5R8Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_preprocessed_data():\n",
        "    encoder_input, decoder_input, decoder_target = [], [], []\n",
        "\n",
        "    with open(\"fra.txt\", \"r\") as lines:\n",
        "        for i, line in enumerate(lines):\n",
        "            src_line, tar_line, _ = line.strip().split(\"\\t\")\n",
        "\n",
        "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
        "\n",
        "            tar_line = preprocess_sentence(tar_line)\n",
        "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
        "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
        "\n",
        "            encoder_input.append(src_line)\n",
        "            decoder_input.append(tar_line_in)\n",
        "            decoder_target.append(tar_line_out)\n",
        "\n",
        "    return encoder_input, decoder_input, decoder_target"
      ],
      "metadata": {
        "id": "KlGOUuk96Iz6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()"
      ],
      "metadata": {
        "id": "Z1faSFoK7ssp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sents):\n",
        "    word_list = []\n",
        "    for sent in sents:\n",
        "        for word in sent:\n",
        "            word_list.append(word)\n",
        "    word_counts = Counter(word_list)\n",
        "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    word_to_index = {}\n",
        "    word_to_index[\"<PAD>\"] = 0\n",
        "    word_to_index[\"<UNK>\"] = 1\n",
        "\n",
        "    for index, word in enumerate(vocab):\n",
        "        word_to_index[word] = index + 2\n",
        "\n",
        "    return word_to_index"
      ],
      "metadata": {
        "id": "zV8v3tI08X4v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = build_vocab(sents_en_in)\n",
        "tar_vocab = build_vocab(sents_fra_in+ sents_fra_out)\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "tar_vocab_size = len(tar_vocab)\n",
        "\n",
        "print(src_vocab_size)\n",
        "print(tar_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUOJ2NVH85zi",
        "outputId": "020bbdc1-1f25-43e3-98b8-36f1886349dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16331\n",
            "26065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_src ={v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "def texts_to_sequences(sents,word_to_index):\n",
        "  encoded_X_data =[]\n",
        "  for sent in tqdm(sents):\n",
        "    index_sequences =[]\n",
        "    for word in sent:\n",
        "      try:\n",
        "        index_sequences.append(word_to_index[word])\n",
        "      except KeyError:\n",
        "        index_sequences.append(word_to_index[\"<UNK>\"])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data\n"
      ],
      "metadata": {
        "id": "ylj8ZzeS9Mmp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
        "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
        "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)\n",
        "\n",
        "for i, (item1,item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
        "  print(f\"index:{i}   {item1} -> {item2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGh9x3Eq92iR",
        "outputId": "4bc116b9-b46e-4de9-b236-eb8b5834df12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 237838/237838 [00:00<00:00, 310802.44it/s]\n",
            "100%|██████████| 237838/237838 [00:00<00:00, 270785.29it/s]\n",
            "100%|██████████| 237838/237838 [00:00<00:00, 733057.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index:0   ['go'] -> [51]\n",
            "index:1   ['go'] -> [51]\n",
            "index:2   ['go'] -> [51]\n",
            "index:3   ['go'] -> [51]\n",
            "index:4   ['hi'] -> [2597]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sentences, max_len=None):\n",
        "  if max_len is None:\n",
        "    max_len = max([len(sentence) for sentence in sentences])\n",
        "  features = np.zeros((len(sentences),max_len),dtype=int)\n",
        "  for index, sentence in enumerate(sentences):\n",
        "    if len(sentence) !=0:\n",
        "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
        "  return features"
      ],
      "metadata": {
        "id": "6Gi665ZF-AjC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = pad_sequences(encoder_input)\n",
        "decoder_input = pad_sequences(decoder_input)\n",
        "decoder_target = pad_sequences(decoder_target)"
      ],
      "metadata": {
        "id": "-2KlRjKf_U39"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input.shape)\n",
        "print(decoder_input.shape)\n",
        "print(decoder_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVdHLoR-_YMT",
        "outputId": "46a2ba9a-1eb0-44ba-cc09-18e69a54f505"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(237838, 65)\n",
            "(237838, 69)\n",
            "(237838, 69)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = np.arange(encoder_input.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi4Wy48b_e_p",
        "outputId": "7ad48114-af19-40fb-d861-d410748c3f14"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[134400  74809 134942 ...   6638 144837  12499]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input= encoder_input[indices]\n",
        "decoder_input= decoder_input[indices]\n",
        "decoder_target= decoder_target[indices]"
      ],
      "metadata": {
        "id": "C3LHNJEG_dz-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([index_to_src[word] for word in encoder_input[6242]])\n",
        "print([index_to_tar[word] for word in decoder_input[6242]])\n",
        "print([index_to_tar[word] for word in decoder_target[6242]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Vdhc2p_oNe",
        "outputId": "db705c86-b4d1-41a3-cb42-ebf576c1d91e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'don', 't', 'take', 'it', 'personally', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['<sos>', 'je', 'ne', 'le', 'prends', 'pas', 'personnellement', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['je', 'ne', 'le', 'prends', 'pas', 'personnellement', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_of_val =int(100000*0.1)\n"
      ],
      "metadata": {
        "id": "7EfxCiIm_8n9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train = encoder_input[:-n_of_val]\n",
        "decoder_input_train = decoder_input[:-n_of_val]\n",
        "decoder_target_train = decoder_target[:-n_of_val]\n",
        "\n",
        "encoder_input_test = encoder_input[-n_of_val:]\n",
        "decoder_input_test = decoder_input[-n_of_val:]\n",
        "decoder_target_test = decoder_target[-n_of_val:]"
      ],
      "metadata": {
        "id": "6hxtlpa-BTsU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input_train.shape)\n",
        "print(decoder_input_train.shape)\n",
        "print(decoder_target_train.shape)\n",
        "print(encoder_input_test.shape)\n",
        "print(decoder_input_test.shape)\n",
        "print(decoder_target_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwvTlmzpBfPI",
        "outputId": "69c25323-78fa-434a-bc2c-80b637f95661"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(227838, 65)\n",
            "(227838, 69)\n",
            "(227838, 69)\n",
            "(10000, 65)\n",
            "(10000, 69)\n",
            "(10000, 69)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "embadding_dim = 256\n",
        "hidden_units = 256\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, src_vocab_size, embadding_dim, hidden_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(src_vocab_size, embadding_dim, padding_idx=0)\n",
        "    self.lstm = nn.LSTM(embadding_dim, hidden_units, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    _, (hidden, cell)= self.lstm(x)\n",
        "    return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, tar_vocab_size, embadding_dim, hidden_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(tar_vocab_size, embadding_dim, padding_idx=0)\n",
        "    self.lstm = nn.LSTM(embadding_dim, hidden_units, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    x = self.embedding(x)\n",
        "    output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
        "    output = self.fc(output)\n",
        "    return output, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Ensure hidden and cell states are contiguous before passing to the decoder\n",
        "        hidden = hidden.contiguous()\n",
        "        cell = cell.contiguous()\n",
        "\n",
        "        output, _, _ = self.decoder(trg, hidden, cell)\n",
        "        return output\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embadding_dim, hidden_units)\n",
        "decoder = Decoder(tar_vocab_size, embadding_dim, hidden_units)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Uvw-WOSjDPeO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsNpC2wID-gu",
        "outputId": "cab2187a-c343-45f1-96a4-ae44b1084f4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(16331, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(26065, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "    (fc): Linear(in_features=256, out_features=26065, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, dataloader, loss_function, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoder_input, decoder_input, decoder_target in dataloader:\n",
        "            encoder_input = encoder_input.to(device)\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            decoder_target = decoder_target.to(device)\n",
        "\n",
        "            outputs = model(encoder_input, decoder_input)\n",
        "            loss = loss_function(\n",
        "                outputs.view(-1, tar_vocab_size),\n",
        "                decoder_target.view(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            mask = decoder_target != 0  # padding 제외\n",
        "            preds = outputs.argmax(dim=2)\n",
        "            total_correct += (preds == decoder_target)[mask].sum().item()\n",
        "            total_count += mask.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_count if total_count > 0 else 0\n",
        "    model.train()  # 평가 후 다시 학습 모드로\n",
        "    return avg_loss, accuracy\n"
      ],
      "metadata": {
        "id": "-itjaDv0FG4o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
        "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
        "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
        "\n",
        "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
        "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
        "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "vEXOwSFsFeuR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs =10\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq6iCirHGRNt",
        "outputId": "c0c7b6ba-a219-4759-ee31-0e654a56f671"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(16331, 256, padding_idx=0)\n",
              "    (lstm): LSTM(256, 256, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(26065, 256, padding_idx=0)\n",
              "    (lstm): LSTM(256, 256, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=26065, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss =float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  for encoder_input, decoder_input, decoder_target in train_dataloader:\n",
        "    encoder_input = encoder_input.to(device)\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    decoder_target = decoder_target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(encoder_input, decoder_input)\n",
        "\n",
        "    loss= loss_function(outputs.view(-1, tar_vocab_size), decoder_target.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
        "  val_loss, val_acc = evaluation(model, test_dataloader, loss_function, device)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}\")\n",
        "  print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y5Qu2p5GW-_",
        "outputId": "abe1e8f3-7f18-40b0-ef99-934cd4e95599"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.3502, Train Acc: 0.4543\n",
            "Epoch: 2\n",
            "Train Loss: 0.2457, Train Acc: 0.5756\n",
            "Epoch: 3\n",
            "Train Loss: 0.1903, Train Acc: 0.6451\n",
            "Epoch: 4\n",
            "Train Loss: 0.1554, Train Acc: 0.6948\n",
            "Epoch: 5\n",
            "Train Loss: 0.1325, Train Acc: 0.7293\n",
            "Epoch: 6\n",
            "Train Loss: 0.1160, Train Acc: 0.7580\n",
            "Epoch: 7\n",
            "Train Loss: 0.1037, Train Acc: 0.7797\n",
            "Epoch: 8\n",
            "Train Loss: 0.0934, Train Acc: 0.7980\n",
            "Epoch: 9\n",
            "Train Loss: 0.0855, Train Acc: 0.8119\n",
            "Epoch: 10\n",
            "Train Loss: 0.0794, Train Acc: 0.8240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape)  # (batch, seq_len, vocab_size)\n",
        "print(decoder_target.shape)  # (batch, seq_len)\n",
        "print(tar_vocab_size)\n"
      ],
      "metadata": {
        "id": "-4Q58dwBKME3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e61d10-f196-4c53-cc6d-7de67cbca536"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([62, 69, 26065])\n",
            "torch.Size([62, 69])\n",
            "26065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.to(device)\n",
        "\n",
        "val_loss, val_acc = evaluation(model, test_dataloader, loss_function, device)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "WLJv43NwHFuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b14df0-740a-4fcd-9640-896c6de8eb4d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2041, Validation Acc: 0.6652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_src ={v: k for k, v in src_vocab.items()}\n",
        "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
        "\n",
        "def seq_to_src(input_seq):\n",
        "  sentence=''\n",
        "  for encoded_word in input_seq:\n",
        "    if(encoded_word!=0):\n",
        "      sentence=sentence+index_to_src[encoded_word]+' '\n",
        "  return sentence\n",
        "\n",
        "def seq_to_tar(input_seq):\n",
        "  sentence=''\n",
        "  for encoded_word in input_seq:\n",
        "    if (encoded_word!=0 and encoded_word!= tar_vocab['<sos>'] and encoded_word!= tar_vocab['<eos>']):\n",
        "      sentence=sentence+index_to_tar[encoded_word]+' '\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "T0orpJtKHTBm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq, model, max_output_len, tar_vocab, index_to_tar):\n",
        "    # 1. 모델을 평가 모드로 설정\n",
        "    model.eval()\n",
        "\n",
        "    # 입력 시퀀스를 텐서로 변환하고 디바이스로 이동\n",
        "    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 인코더를 통해 초기 hidden, cell state 얻기\n",
        "    with torch.no_grad(): # 추론 시에는 gradient 계산이 필요 없음\n",
        "        hidden, cell = model.encoder(encoder_inputs)\n",
        "\n",
        "    # 2. <sos> 토큰의 실제 인덱스를 tar_vocab에서 가져와 시작점으로 사용\n",
        "    decoder_input = torch.tensor([tar_vocab['<sos>']], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    decoded_tokens = []\n",
        "\n",
        "    # 설정된 최대 길이만큼 반복하여 단어 생성\n",
        "    for _ in range(max_output_len):\n",
        "        with torch.no_grad():\n",
        "            # 디코더로 다음 단어 예측\n",
        "            output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "        # 가장 확률이 높은 단어의 인덱스 추출\n",
        "        output_token = output.argmax(dim=-1).item()\n",
        "\n",
        "        # 3. <eos> 토큰의 실제 인덱스를 만나면 번역 종료\n",
        "        if output_token == tar_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "        decoded_tokens.append(output_token)\n",
        "\n",
        "        # 현재 예측된 단어를 다음 시점의 입력으로 사용\n",
        "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 정수 시퀀스를 문자열로 변환하여 반환\n",
        "    return ' '.join(index_to_tar.get(token, '<unk>') for token in decoded_tokens)\n",
        "\n",
        "# ===== 함수 호출 부분도 아래와 같이 수정해야 합니다 =====\n",
        "\n",
        "for seq_index in [3, 50, 100, 300, 1001]:\n",
        "    input_seq = encoder_input_train[seq_index]\n",
        "\n",
        "    # 수정한 함수에 맞게 인자 전달\n",
        "    translated_text = decode_sequence(input_seq, model, 20, tar_vocab, index_to_tar)\n",
        "\n",
        "    print(\"입력 문장:\", seq_to_src(encoder_input_train[seq_index]))\n",
        "    print(\"정답 문장:\", seq_to_tar(decoder_input_train[seq_index]))\n",
        "    print(\"번역 문장:\", translated_text)\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "_MBrnJ8XH_7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e631160-083b-4921-a9ec-48b4f5c9c7a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: i know you re not like that \n",
            "정답 문장: je sais que vous n etes pas comme ca \n",
            "번역 문장: je sais que vous n etes pas comme ca\n",
            "==================================================\n",
            "입력 문장: don t do too much at the same time \n",
            "정답 문장: n en faites pas trop en meme temps \n",
            "번역 문장: ne fais pas trop de bruit pour toujours\n",
            "==================================================\n",
            "입력 문장: i lost my passport i ll have to get a new one \n",
            "정답 문장: j ai perdu mon passeport je devrai en refaire faire un \n",
            "번역 문장: j ai perdu mon passeport je devrai en avoir un nouveau\n",
            "==================================================\n",
            "입력 문장: they let me go \n",
            "정답 문장: elles m ont laissee partir \n",
            "번역 문장: ils m ont laissee partir\n",
            "==================================================\n",
            "입력 문장: somebody has stolen my suitcase \n",
            "정답 문장: quelqu un a vole ma valise \n",
            "번역 문장: quelqu un a vole ma valise\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}